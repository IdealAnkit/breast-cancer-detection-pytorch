{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŽ—ï¸ Breast Cancer Detection using PyTorch (nn.Module)\n",
        "\n",
        "This notebook implements a **Binary Classification Model** using **Logistic Regression** structured with `torch.nn.Module`, following industry-standard practices.\n",
        "\n",
        "### ðŸš€ Complete Pipeline:\n",
        "1. **Import Libraries**: Load necessary Python libraries (`torch`, `nn`, `optim`).\n",
        "2. **Load Dataset**: Fetch the Breast Cancer dataset from a remote URL.\n",
        "3. **Data Cleaning**: Remove unused columns like `id` and `Unnamed: 32`.\n",
        "4. **Train-Test Split**: Divide data into training (80%) and testing (20%) sets.\n",
        "5. **Feature Scaling**: Normalize features using `StandardScaler` for better convergence.\n",
        "6. **Label Encoding**: Convert categorical labels ('M', 'B') into numerical values (1, 0).\n",
        "7. **Convert to PyTorch Tensors**: Prepare data for PyTorch training.\n",
        "8. **Define Model**: Implement a custom class inheriting from `nn.Module`.\n",
        "9. **Define Loss & Optimizer**: Use built-in `nn.BCELoss` and `torch.optim.SGD`.\n",
        "10. **Training Loop**: Train the model using the standard PyTorch workflow.\n",
        "11. **Evaluation**: Test the model's accuracy on unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1ï¸âƒ£ Import Required Libraries\n",
        "\n",
        "We import:\n",
        "- `torch`: The core PyTorch library.\n",
        "- `torch.nn`: For building neural network layers (`nn.Linear`, `nn.Sigmoid`).\n",
        "- `torch.optim`: For optimization algorithms (`SGD`).\n",
        "- `numpy`, `pandas`: For data manipulation.\n",
        "- `sklearn`: For preprocessing and splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2ï¸âƒ£ Load Dataset\n",
        "\n",
        "We load the dataset directly from the raw GitHub URL.\n",
        "The dataset contains features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3ï¸âƒ£ Data Cleaning\n",
        "\n",
        "We inspect the dataset and remove columns that are not useful for training, specifically:\n",
        "- `id`: Unique identifier for each sample.\n",
        "- `Unnamed: 32`: An empty column often created during CSV reading due to trailing commas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove unnecessary columns\n",
        "df.drop(columns=['id', 'Unnamed: 32'], inplace=True)\n",
        "\n",
        "print(\"Shape after dropping columns:\", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4ï¸âƒ£ Train-Test Split\n",
        "\n",
        "We separate the features (`X`) from the target variable (`y`).\n",
        "- `X`: All columns except diagnosis.\n",
        "- `y`: The diagnosis column ('M' or 'B').\n",
        "\n",
        "Then, we split the data into training (80%) and testing (20%) sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.iloc[:, 1:]      # All features (excluding diagnosis at index 0)\n",
        "y = df.iloc[:, 0]       # diagnosis column\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training Features Shape:\", X_train.shape)\n",
        "print(\"Testing Features Shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5ï¸âƒ£ Feature Scaling\n",
        "\n",
        "Standardization calculates the mean and standard deviation for each feature and scales the data so that the mean is 0 and standard deviation is 1.\n",
        "This is crucial for neural networks to converge faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on training set only to avoid data leakage\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(\"First 3 rows of Scaled Training Data:\\n\", X_train[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6ï¸âƒ£ Label Encoding\n",
        "\n",
        "The target variable has categorical values:\n",
        "- **M** (Malignant) -> Encoded as **1**\n",
        "- **B** (Benign) -> Encoded as **0**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = LabelEncoder()\n",
        "\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_test = encoder.transform(y_test)\n",
        "\n",
        "print(\"Encoded Labels Example (First 10):\", y_train[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7ï¸âƒ£ Convert to PyTorch Tensors\n",
        "\n",
        "PyTorch works with Tensors. We convert our numpy arrays to PyTorch tensors and ensure they are of type `float`.\n",
        "We also reshape the target labels to be column vectors `(N, 1)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_tensor = torch.from_numpy(X_train).float()\n",
        "X_test_tensor  = torch.from_numpy(X_test).float()\n",
        "\n",
        "# Reshape labels to (N, 1)\n",
        "y_train_tensor = torch.from_numpy(y_train).float().view(-1, 1)\n",
        "y_test_tensor  = torch.from_numpy(y_test).float().view(-1, 1)\n",
        "\n",
        "print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
        "print(\"y_train_tensor shape:\", y_train_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8ï¸âƒ£ Define Neural Network Model (`nn.Module`)\n",
        "\n",
        "We define a simple Logistic Regression model class inheriting from `nn.Module`.\n",
        "\n",
        "**Architecture**:\n",
        "- **Linear Layer**: Maps input features to a single output value.\n",
        "- **Sigmoid Activation**: squashes the output between 0 and 1 (probability).\n",
        "\n",
        "**Note**: We do NOT define the loss function inside the class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MySimpleNN(nn.Module):\n",
        "\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "\n",
        "        # Linear Layer\n",
        "        self.linear = nn.Linear(num_features, 1)\n",
        "        \n",
        "        # Sigmoid activation\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, features):\n",
        "        # Linear transformation\n",
        "        out = self.linear(features)\n",
        "        \n",
        "        # Apply sigmoid\n",
        "        out = self.sigmoid(out)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9ï¸âƒ£ Training Pipeline Setup\n",
        "\n",
        "We use specific hyperparameters and built-in Torch tools:\n",
        "- **Loss Function**: `nn.BCELoss()` - Binary Cross Entropy Loss.\n",
        "- **Optimizer**: `torch.optim.SGD` - Stochastic Gradient Descent.\n",
        "- **Learning Rate**: 0.1\n",
        "- **Epochs**: 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.1\n",
        "epochs = 25\n",
        "\n",
        "# Create Model Instance\n",
        "model = MySimpleNN(X_train_tensor.shape[1])\n",
        "\n",
        "# Define Loss Function (Built-in)\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"Model, Loss, and Optimizer initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”Ÿ Training Loop\n",
        "\n",
        "The standard PyTorch training loop consists of 5 main steps:\n",
        "1. **Forward Pass**: Compute prediction `y_pred` from input `X`.\n",
        "2. **Calculate Loss**: Compare `y_pred` with true labels `y`.\n",
        "3. **Zero Gradients**: Clear old gradients from the previous step.\n",
        "4. **Backward Pass**: Compute gradients for all parameters (`loss.backward()`).\n",
        "5. **Optimizer Step**: Update model parameters (`optimizer.step()`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Starting Training...\\n\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # ------------------\n",
        "    # Forward pass\n",
        "    # ------------------\n",
        "    y_pred = model(X_train_tensor)\n",
        "\n",
        "    # ------------------\n",
        "    # Loss calculate\n",
        "    # ------------------\n",
        "    loss = loss_function(y_pred, y_train_tensor.view(-1, 1))\n",
        "\n",
        "    # ------------------\n",
        "    # Clear gradients\n",
        "    # ------------------\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # ------------------\n",
        "    # Backward pass\n",
        "    # ------------------\n",
        "    loss.backward()\n",
        "\n",
        "    # ------------------\n",
        "    # Parameters update\n",
        "    # ------------------\n",
        "    optimizer.step()\n",
        "\n",
        "    # ------------------\n",
        "    # Print loss\n",
        "    # ------------------\n",
        "    print(f'Epoch: {epoch + 1}, Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1ï¸âƒ£1ï¸âƒ£ Model Evaluation\n",
        "\n",
        "We evaluate the trained model on the unseen Test data.\n",
        "- Compute predictions on `X_test_tensor`.\n",
        "- Threshold probabilities at 0.5 to convert to binary classes (0 or 1).\n",
        "- Compare predicted classes with true labels (`y_test_tensor`) to calculate accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # Get probabilities for test set\n",
        "    y_pred = model(X_test_tensor)\n",
        "    \n",
        "    # Convert probabilities to 0/1 classes\n",
        "    y_pred_cls = (y_pred > 0.5).float()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (y_pred_cls == y_test_tensor.view(-1, 1)).float().mean()\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy.item():.4f}\")\n",
        "    print(f\"Test Accuracy Score: {accuracy.item() * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}